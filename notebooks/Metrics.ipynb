{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe4e8df-f5ff-404d-81db-623b7cf78cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser/notebooks/sketch-to-artwork\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5909c9f-a352-411b-a816-43025348c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "\u001b[K     |████████████████████████████████| 329 kB 26.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /anaconda/envs/taming/lib/python3.8/site-packages (from torchmetrics) (21.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /anaconda/envs/taming/lib/python3.8/site-packages (from torchmetrics) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /anaconda/envs/taming/lib/python3.8/site-packages (from torchmetrics) (1.19.2)\n",
      "Requirement already satisfied: future in /anaconda/envs/taming/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /anaconda/envs/taming/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /anaconda/envs/taming/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/taming/lib/python3.8/site-packages (from packaging->torchmetrics) (2.4.7)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# Isntall necessary packages\n",
    "# !pip install torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582826f5-e14c-49ea-a65c-95516b95ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchmetrics import FID, LPIPS\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from taming.data.wikiart import WikiartEdgesTrain, WikiartEdgesTest\n",
    "from taming.data.base import ImagePaths\n",
    "\n",
    "class CustomDataset(ImagePaths):\n",
    "    def __init__(self, root):\n",
    "        paths = os.listdir(root)\n",
    "        paths = [os.path.join(root, fname) for fname in paths if fname[len(fname)-3:] == \"png\"]\n",
    "        super().__init__(paths, size=256)\n",
    "\n",
    "def convert_to_uint8(images_float):\n",
    "    return (torch.clamp(images_float * 0.5 + 0.5, 0., 1.) * 255.).to(dtype=torch.uint8)\n",
    "\n",
    "dataset_trn = WikiartEdgesTrain(256, \"datasets/wikiart_train.txt\")\n",
    "dataset_gen = CustomDataset(root='datasets/wikiart_generated_256')\n",
    "dataset_val = WikiartEdgesTest(256, \"datasets/wikiart_val.txt\")\n",
    "\n",
    "gen_loader = torch.utils.data.DataLoader(dataset_gen, batch_size=4, num_workers=16)\n",
    "trn_loader = torch.utils.data.DataLoader(dataset_trn, batch_size=4, num_workers=16)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=4, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65fbe6f-4cd9-49b5-a942-14f72c99582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587/587 [00:34<00:00, 17.05it/s]\n",
      "100%|██████████| 35/35 [00:01<00:00, 24.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 194.244492\n"
     ]
    }
   ],
   "source": [
    "# FID\n",
    "\n",
    "fid_module = FID(feature=2048).to('cuda')\n",
    "\n",
    "for batch in tqdm(trn_loader):\n",
    "    imgs = batch['image'].permute(0, 3, 1, 2)\n",
    "    imgs = convert_to_uint8(imgs).cuda()\n",
    "    fid_module.update(imgs, real=True)\n",
    "\n",
    "for batch in tqdm(gen_loader):\n",
    "    imgs = batch['image'].permute(0, 3, 1, 2)\n",
    "    imgs = convert_to_uint8(imgs).cuda()\n",
    "    fid_module.update(imgs, real=False)\n",
    "\n",
    "fid = fid_module.compute().item()\n",
    "print(f'FID: {fid:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33b8ceb-2133-4745-a3ae-d033d5058f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [01:24,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPIPS: 0.802710\n"
     ]
    }
   ],
   "source": [
    "gen_loader = torch.utils.data.DataLoader(dataset_gen, batch_size=1, num_workers=16)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1, num_workers=16)\n",
    "\n",
    "# LPIPS\n",
    "lpips = LPIPS(net_type='vgg')\n",
    "lpips_val = []\n",
    "for batch1, batch2 in tqdm(zip(gen_loader, val_loader)):\n",
    "    imgs = batch1['image'].permute(0, 3, 1, 2)\n",
    "    imgs2 = batch2['image'].permute(0, 3, 1, 2)\n",
    "    temp = lpips(imgs, imgs2).detach().cpu().numpy()\n",
    "    lpips_val = np.append(lpips_val, temp)\n",
    "    \n",
    "print(f'LPIPS: {np.mean(lpips_val):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0212a5d-194c-4e95-b29a-1da44fc99119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taming",
   "language": "python",
   "name": "conda-env-taming-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
